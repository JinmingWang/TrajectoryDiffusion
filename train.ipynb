{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/TrajectoryDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Dataset.DidiDataset import DidiTrajectoryDataset, collectFunc\n",
    "from Models.TrajUNet import TrajUNet\n",
    "from DiffusionManager import DiffusionManager\n",
    "from Utils import MovingAverage, saveModel, loadModel, exportONNX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "from os import makedirs\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "chengdu_nov_dataset_args = {\n",
    "    \"dataset_root\": \"E:/Data/Didi/chengdu/nov\",\n",
    "    \"traj_length\": 200,\n",
    "    \"feature_mean\": [21599.4980, 104.0789535914567, 30.680879399098956],    # time lon lat\n",
    "    \"feature_std\": [12470.9102, 0.0032705687484356773, 0.018204423046522103],\n",
    "}\n",
    "\n",
    "xian_nov_dataset_args = {\n",
    "    \"dataset_root\": \"E:/Data/Didi/xian/nov\",\n",
    "    \"traj_length\": 200,\n",
    "    \"feature_mean\": [21599.4980, 108.950773428688, 34.24354179925547],    # time lon lat\n",
    "    \"feature_std\": [12470.9102, 0.02129110045580343, 0.019358855648211895],\n",
    "}\n",
    "\n",
    "diffusion_args = {\n",
    "    \"min_beta\": 0.0001,\n",
    "    \"max_beta\": 0.05,\n",
    "    \"max_diffusion_step\": 500,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    \"channel_schedule\": [128, 128, 256, 512, 1024],\n",
    "    \"diffusion_steps\": diffusion_args[\"max_diffusion_step\"],\n",
    "    \"res_blocks\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "init_lr = 5e-4\n",
    "lr_reduce_factor = 0.5\n",
    "lr_reduce_patience = 2000\n",
    "\n",
    "# Colab can have 51GB RAM or 12.7GB RAM, GPU is Tesla T4 which has 15GB RAM\n",
    "files_per_part = 1\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "log_interval = 100\n",
    "save_interval = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DidiTrajectoryDataset(**xian_nov_dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajUNet(**model_args).cuda()\n",
    "model.train()\n",
    "diff_manager = DiffusionManager(**diffusion_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=init_lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_reduce_factor, patience=lr_reduce_patience, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "makedirs(f\"Runs/{start_time}\")\n",
    "writer = SummaryWriter(f\"Runs/{start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "If your memory >= 16GB, you can load the dataset in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading E:/Data/Didi/xian/nov\\gps_20161101.pt\n",
      "Loading E:/Data/Didi/xian/nov\\gps_20161102.pt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.loadNextFiles(dataset.n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_it = 0\n",
    "\n",
    "mov_avg_loss = MovingAverage(log_interval)\n",
    "\n",
    "for e in range(epochs):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collectFunc)\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {e}')\n",
    "    for traj_0, cat_attr, num_attr, times in pbar:\n",
    "        # Diffusion forward\n",
    "        t = torch.randint(0, diffusion_args[\"max_diffusion_step\"], (traj_0.shape[0],)).cuda()\n",
    "        epsilon = torch.randn_like(traj_0).cuda()\n",
    "        traj_t = diff_manager.diffusionForward(traj_0, t, epsilon)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epsilon_pred = model(traj_t, t, cat_attr, num_attr)\n",
    "        loss = loss_func(epsilon_pred, epsilon)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(float(mov_avg_loss))\n",
    "\n",
    "        global_it += 1\n",
    "        mov_avg_loss << loss.item()\n",
    "        pbar.set_postfix_str(f'Loss: {mov_avg_loss:.5f}')\n",
    "\n",
    "        if global_it % log_interval == 0:\n",
    "            writer.add_scalar('Loss', float(mov_avg_loss), global_it)\n",
    "            writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], global_it)\n",
    "\n",
    "        if global_it % save_interval == 0:\n",
    "            saveModel(model, f\"Runs/{start_time}/{model.__class__.__name__}_{global_it}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "If your memory < 16GB, you may want to load the dataset in parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading E:/Data/Didi/xian/nov\\gps_20161103.pt\n",
      "Loading E:/Data/Didi/xian/nov\\gps_20161104.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 File 2/30:   0%|          | 0/3603 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\TrajectoryDiffusion\\train.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X20sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, collate_fn\u001b[39m=\u001b[39mcollectFunc)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X20sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pbar \u001b[39m=\u001b[39m tqdm(dataloader, desc\u001b[39m=\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m File \u001b[39m\u001b[39m{\u001b[39;00mn_files_load\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mtotal_num_files\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m traj_0, cat_attr, num_attr, times \u001b[39min\u001b[39;00m pbar:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Diffusion forward\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, diffusion_args[\u001b[39m\"\u001b[39m\u001b[39mmax_diffusion_step\u001b[39m\u001b[39m\"\u001b[39m], (traj_0\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m],))\u001b[39m.\u001b[39mcuda()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     epsilon \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn_like(traj_0)\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:530\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    528\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    529\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> 530\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    531\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    532\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    533\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    534\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:570\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    568\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    569\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 570\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    572\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32me:\\Projects\\TrajectoryDiffusion\\Dataset\\DidiDataset.py:145\u001b[0m, in \u001b[0;36mDidiTrajectoryDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m# lon_lat: (N, 2), times: (N,)\u001b[39;00m\n\u001b[0;32m    144\u001b[0m lon_lat, times \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset_part[index]\n\u001b[1;32m--> 145\u001b[0m times \u001b[39m=\u001b[39m times\u001b[39m.\u001b[39;49mto(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice)\n\u001b[0;32m    146\u001b[0m lon_lat \u001b[39m=\u001b[39m lon_lat\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    148\u001b[0m \u001b[39m# times: number of minutes since 2016-10-01 00:00:00\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39m# The absolute time may not be useful, but daytime, day of the month may be useful\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "global_it = 0\n",
    "\n",
    "mov_avg_loss = MovingAverage(log_interval)\n",
    "\n",
    "for e in range(epochs):\n",
    "    n_files_load = 0\n",
    "    total_num_files = dataset.n_files\n",
    "    while dataset.loadNextFiles(files_per_part):\n",
    "        n_files_load  = min(n_files_load + files_per_part, total_num_files)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collectFunc)\n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {e} File {n_files_load}/{total_num_files}')\n",
    "        for traj_0, cat_attr, num_attr, times in pbar:\n",
    "            # Diffusion forward\n",
    "            t = torch.randint(0, diffusion_args[\"max_diffusion_step\"], (traj_0.shape[0],)).cuda()\n",
    "            epsilon = torch.randn_like(traj_0).cuda()\n",
    "            traj_t = diff_manager.diffusionForward(traj_0, t, epsilon)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epsilon_pred = model(traj_t, t, cat_attr, num_attr)\n",
    "            loss = loss_func(epsilon_pred, epsilon)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step(float(mov_avg_loss))\n",
    "\n",
    "            global_it += 1\n",
    "            mov_avg_loss << loss.item()\n",
    "            pbar.set_postfix_str(f'Loss: {mov_avg_loss:.5f}')\n",
    "\n",
    "            if global_it % log_interval == 0:\n",
    "                writer.add_scalar('Loss', float(mov_avg_loss), global_it)\n",
    "                writer.add_scalar('Learning Rate', optimizer.param_groups[0]['lr'], global_it)\n",
    "\n",
    "            if global_it % save_interval == 0:\n",
    "                saveModel(model, f\"Runs/{start_time}/{model.__class__.__name__}_{global_it}.pth\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
