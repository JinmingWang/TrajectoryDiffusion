{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "%cd /content/drive/MyDrive/TrajectoryDiffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from Dataset.DidiDataset import DidiTrajectoryDataset, collectFunc\n",
    "from Models.TrajUNet import TrajUNet\n",
    "from DiffusionManager import DiffusionManager\n",
    "from Utils import MovingAverage, saveModel, loadModel, exportONNX\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datetime import datetime\n",
    "from os import makedirs\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_args = {\n",
    "    \"dataset_root\": \"E:/Data/Didi/xian/nov\",\n",
    "    \"traj_length\": 200,\n",
    "    \"lat_mean\": 108.95038635089452,\n",
    "    \"lat_std\": 0.02245034359640356,\n",
    "    \"lon_mean\": 34.242824702030525,\n",
    "    \"lon_std\": 0.019082048008517993\n",
    "}\n",
    "\n",
    "diffusion_args = {\n",
    "    \"min_beta\": 0.0001,\n",
    "    \"max_beta\": 0.005,\n",
    "    \"max_diffusion_step\": 300,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "    \"channel_schedule\": [128, 128, 256, 512, 1024],\n",
    "    \"diffusion_steps\": diffusion_args[\"max_diffusion_step\"],\n",
    "    \"res_blocks\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "init_lr = 1e-3\n",
    "lr_reduce_factor = 0.5\n",
    "lr_reduce_patience = 2000\n",
    "\n",
    "# Colab can have 51GB RAM or 12.7GB RAM, GPU is Tesla T4 which has 15GB RAM\n",
    "files_per_part = 2\n",
    "batch_size = 32\n",
    "epochs = 100\n",
    "log_interval = 100\n",
    "save_interval = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DidiTrajectoryDataset(**dataset_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TrajUNet(**model_args).cuda()\n",
    "diff_manager = DiffusionManager(**diffusion_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=init_lr)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_reduce_factor, patience=lr_reduce_patience, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "makedirs(f\"Runs/{start_time}\")\n",
    "writer = SummaryWriter(f\"Runs/{start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "If your memory >= 16GB, you can load the dataset in one go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.loadNextFiles(dataset.n_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_it = 0\n",
    "\n",
    "mov_avg_loss = MovingAverage(log_interval)\n",
    "\n",
    "for e in range(epochs):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collectFunc)\n",
    "    pbar = tqdm(dataloader, desc=f'Epoch {e}')\n",
    "    for traj_0, attr in pbar:\n",
    "        # Diffusion forward\n",
    "        t = torch.randint(0, diffusion_args[\"max_diffusion_step\"], (traj_0.shape[0],)).cuda()\n",
    "        epsilon = torch.randn_like(traj_0).cuda()\n",
    "        traj_t = diff_manager.diffusionForward(traj_0, t, epsilon)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        epsilon_pred = model(traj_t, t, attr)\n",
    "        loss = loss_func(epsilon_pred, epsilon)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(float(mov_avg_loss))\n",
    "\n",
    "        global_it += 1\n",
    "        mov_avg_loss << loss.item()\n",
    "        pbar.set_postfix_str(f'Loss: {mov_avg_loss:.5f}')\n",
    "\n",
    "        if global_it % log_interval == 0:\n",
    "            writer.add_scalar('Loss', float(mov_avg_loss), global_it)\n",
    "\n",
    "        if global_it % save_interval == 0:\n",
    "            saveModel(model, f\"Runs/{start_time}/{model.__class__.__name__}_{global_it}.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train\n",
    "If your memory < 16GB, you may want to load the dataset in parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading E:/Data/Didi/xian/nov\\gps_20161101.pt\n",
      "Loading E:/Data/Didi/xian/nov\\gps_20161102.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0 File 2/30:   8%|â–Š         | 199/2457 [00:20<03:58,  9.48it/s, Loss: 1.00379]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32me:\\Projects\\TrajectoryDiffusion\\train.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X10sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X10sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m global_it \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/Projects/TrajectoryDiffusion/train.ipynb#X10sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m pbar\u001b[39m.\u001b[39mset_postfix_str(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mLoss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "global_it = 0\n",
    "\n",
    "mov_avg_loss = MovingAverage(log_interval)\n",
    "\n",
    "for e in range(epochs):\n",
    "    n_files_load = 0\n",
    "    total_num_files = dataset.n_files\n",
    "    while dataset.loadNextFiles(files_per_part):\n",
    "        n_files_load  = min(n_files_load + files_per_part, total_num_files)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collectFunc)\n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {e} File {n_files_load}/{total_num_files}')\n",
    "        for traj_0, attr in pbar:\n",
    "            # Diffusion forward\n",
    "            t = torch.randint(0, diffusion_args[\"max_diffusion_step\"], (traj_0.shape[0],)).cuda()\n",
    "            epsilon = torch.randn_like(traj_0).cuda()\n",
    "            traj_t = diff_manager.diffusionForward(traj_0, t, epsilon)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            epsilon_pred = model(traj_t, t, attr)\n",
    "            loss = loss_func(epsilon_pred, epsilon)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            global_it += 1\n",
    "            mov_avg_loss << loss.item()\n",
    "            pbar.set_postfix_str(f'Loss: {mov_avg_loss:.5f}')\n",
    "\n",
    "            if global_it % log_interval == 0:\n",
    "                writer.add_scalar('Loss', float(mov_avg_loss), global_it)\n",
    "\n",
    "            if global_it % save_interval == 0:\n",
    "                saveModel(model, f\"Runs/{start_time}/{model.__class__.__name__}_{global_it}.pth\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
